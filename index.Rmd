---
title: "AI in StAtIstics"
date: "`r format(Sys.time(), '%B %d, %Y')`"
#bibliography: [Refs/_ClinVal.bib, Refs/GRAIL_Liu_2020/_GRAIL_Liu_2020.bib,
#               Refs/Liu_2025/_Liu_2025.bib]
csl: ../_csl/cell-numeric.csl
link-citations: true
always_allow_html: yes
output:
  #html_document:
  # for use of .tabset
  rmarkdown::html_document:
    code_folding: hide
    code_download: true
    toc: true
    toc_depth: 2
    # does this have an effect
    fig_caption: yes
    # this has no effect
    number_sections: yes
    # css: ['../_css/pandoc3.css', '../_css/myMargins.css']
---

<!-- 

<p style="background-color: yellow; padding: 5px;">
  <span style="font-weight: bold;">⚠️ Work in Progress:</span> This content is incomplete.
</p>

-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.width = 8,
                      fig.height = 4)
```


```{r index-GlobalOptions, results="hide", include=FALSE, cache=FALSE}

knitr::opts_knit$set(stop_on_error = 2L) #really make it stop
options(knitr.table.format = 'html')

options(stringsAsFactors=F)

 #knitr::dep_auto()

```
<!-- ######################################################################## -->


```{r index-Prelims,  include=FALSE, echo=FALSE, results='hide', message=FALSE} 


FN <- "index"
if(sum(grepl(FN, list.files()))==0) stop("Check FN")

PREFIX <- "index" #- replace by FLOWCELL???

 suppressMessages(require(rmarkdown))
 suppressMessages(require(knitr))

 suppressPackageStartupMessages(require(methods))
 suppressPackageStartupMessages(require(bookdown))

 suppressPackageStartupMessages(require(magrittr))

 # Shotcuts for knitting and rendering while in R session (Invoke interactive R from R/Scripts folder)
 kk <- function(n='') knitr::knit2html(paste("t", n, sep=''), envir=globalenv(),
       output=paste(FN,".html", sep=''))

 rr <- function(n='') rmarkdown::render(paste("t", n, sep=''), envir=globalenv(),
       output_file=paste(FN,".html", sep='')) ##, output_dir='Scripts')

 bb <- function(n='') browseURL(paste(FN,".html", sep=''))

 # The usual shortcuts
 zz <- function(n='') source(paste("t", n, sep=''))

```
<!-- ######################################################################## -->

<br/>

#  Introduction 

**TBC**


<br/>
 

# Terry Speed on AI/Data Science

<br/>

## On Data Science 

His most direct statement on the AI/data science debate:

Speed commented on the data science movement, asking: "Are we doing such a bad job that we need to rename ourselves data scientists to capture the imagination of future students, collaborators, or clients? Are we so lacking in confidence … that we shiver in our shoes the moment a potential usurper appears on the scene? Or, has there really been a fundamental shift around us, so that our old clumsy ways of adapting and evolving are no longer adequate?"

He continued with a defense of statistics' enduring value: "I think we have a great tradition and a great future, both far longer than the concentration span of funding agencies, university faculties, and foundations. … We might miss out on the millions being lavished on data science right now, but that's no reason for us to stop trying to do the best we can at what we do best, something that is far wider and deeper than data science."

<br/>

## On Machine Learning and Statistics 

In a 2006 column and 2016 AMSTAT interview, Speed observed that computer science, machine learning, image analysis, information theory and bioinformatics have all provided future statisticians to statistics departments around the world in recent years, and that "there are misunderstandings on both sides about what statistics and statisticians can and should do." He characterized the data science movement as "a necessary correction for our community" while acknowledging that "there is no doubt in my mind that there are real and important issues here, mixed in with the usual human characteristics of exaggeration (hype), oversimplifying, competition for resources (turf battles), and so on."

<br/>

## Current AI Research 

More recently, Speed has been part of a team at WEHI that developed AI-based methodology to remove unwanted variation from biomedical data, showing his practical engagement with machine learning methods in his genomics work.

Speed's perspective appears to be that of a pragmatic statistician who values what works over ideological debates about methodology, while defending statistics as a discipline with deeper foundations than any single technological wave.

---

## Additional Findings on Terry Speed 

### Background {-}

Terry Speed (born 1943) is an Australian statistician known for: 

- Professor at UC Berkeley (Statistics) and WEHI (Bioinformatics)
- Expert on microarray analysis and statistical genetics
- 2013 Prime Minister's Prize for Science winner
- Long-running IMS Bulletin column "Terence's Stuff" (now discontinued)

<br/>

### On "Principled" Methods  {-}

* 2016 IMS Bulletin column  

Speed objected to describing statistical methods as "principled," arguing: "If a statistical analysis is clearly shown to be effective … it gains nothing from being … principled." He criticized the implication that other approaches are "unprincipled."

<br/>

### On Statistician Perceptions {-}

* 2014 IMS column cited by Bin Yu

Speed quoted an outsider view that statisticians "don't deal with risk, with uncertainty… we're too absolute, we do p-values, confidence intervals, definite things like that." Scientists wanted "actionable insight" not "arcane concerns about mathematical methods."

<br/>

### 2023 WEHI AI Work {-}

Speed's team (with Papenfuss, Molania) developed AI-based methodology to remove unwanted variation from biomedical data, published in Nature Biotechnology. Applied to Cancer Genome Atlas (10,000 patient samples, 33 cancer types).

<br/>

## Summary Characterization

Speed's stance: pragmatic defender of statistics as discipline while acknowledging legitimate critiques. Values what works over ideological purity. Skeptical of hype and rebranding but engaged with ML methods in practice. Sees statistics as having "wider and deeper" foundations than data science trends.

---

# Michael Jordan's Farewell Tour 

I thought the Michael Jordan was the best person to champion sanity
and reason in this period of unconstrained hype and awe.
Here are a few links - there are many more but these came at the
top of search results and I think are representative of how  he thinks and speaks:

* MJ addresses the UC regents in 2023 - https://www.youtube.com/watch?v=AygJUbwCwJc

<p><p/>
* A couple of typical talks:
   - https://www.youtube.com/watch?v=KaFYaFquFLs
   - https://www.youtube.com/watch?v=3zlDHdtSXt4

<p><p/>
* Here is where he explains why he now lives and works in Europe - https://youtu.be/ZJGHhs6fNkY?si=tj8Vpm6Kd5S-5PbC

So it appears that after several years of pushing back on the 
Silicon Valley mindset regarding AI, MJ decided to pack up and
go work with people with a more similar mindset.
So my champion said **F-it** to the prevailing mindset and left.
That doesn't fare too well, for me and other journeyman statisticians,
who just want to provide guidance on how best to collect, analyze and interpret
data  in the face of variability.^[
Believe you me, without a trace of presumption, I can vouch that there are 
countless opportunities for a small dose of statistical judgement to
alleviate ills that may be afflicting an analysis and report.
The last place I worked at was staffed by the brightest young minds.
I mean google smart.  Still I don't think there was a single presentation
that wasn't tainted by ill-advised p-values, analyzing the data on the
wrong scale, wrong objectives (assumed but never explicitly stated),
interpreting model coefficients without examining the fit.  Yet, here I yam.
The kids could have, and would have, very easily incorporated suitable
adjustments to their analyses, but the managers were emphatically uninterested.
When your bonus is to a large extent determined by the calendar dates,
and everyone agrees that methods don't matter, there is literally no point
in improving analyses.]

<br/>

---

# Michael Jordan mentions on Andrew Gelman's blog

## 1. "No, Michael Jordan didn't say that!" (October 23, 2014) {-}
https://statmodeling.stat.columbia.edu/2014/10/23/michael-jordan-didnt-say/

This post discusses a controversy over an IEEE Spectrum interview with Jordan about AI and big data. Bob Carpenter notes that Jordan claimed it was "patently false" to think there is any neuroscience behind current Deep Learning and machine learning techniques, and that we are "not yet in an era" where we can use brain understanding to create intelligent computer systems. The post includes a lively debate in comments between the journalist Lee Gomes and readers about the accuracy of the interview framing.

<br/>

## 2. "The ML uncertainty revolution is … now?" (August 5, 2021){-}
https://statmodeling.stat.columbia.edu/2021/08/05/the-ml-uncertainty-revolution-is-now/

At a workshop on distribution-free uncertainty quantification, Michael Jordan of Berkeley alluded to how uncertainty quantification has long been a niche topic in ML, and still kind of is, but this might be shifting.

<br/>

## 3. "Static sensitivity analysis" (May 29, 2017) {-}
https://statmodeling.stat.columbia.edu/2017/05/29/static-sensitivity-analysis/

Gelman writes: "After this discussion, I pointed Ryan Giordano, Tamara Broderick, and Michael Jordan to Figure 4 of this paper with Bois and Jiang as an example of 'static sensitivity analysis.'" The paper cited is Giordano, Broderick, and Jordan (2018) on sensitivity analysis.

<br/>

## 4. "What are the most important statistical ideas of the past 50 years?" (December 9, 2020) {-}
https://statmodeling.stat.columbia.edu/2020/12/09/what-are-the-most-important-statistical-ideas-of-the-past-50-years/

Bob Carpenter comments: "You have people like Michael Jordan who straddle the fields and produce students straddling the fields like Dave Blei, Tamara Broderick, and Francis Bach. What's funny is that I vividly remember talks by Jordan and Hinton in psychology (CMU psych was big into 'connectionism') in the 1980s and early 90s on neural nets that everyone in CS just yawned through."

<br/>

## 5. "Stan Down Under" (February 15, 2015) {-}
https://statmodeling.stat.columbia.edu/2015/02/15/stan-3/

Bob Carpenter mentions Google search diversity, noting: "you don't get 1000 hits for Michael Jordan the basketball player before the first hit for Michael Jordan the computer scientist (or maybe vice-versa these days?)."

<br/>

## 6. "Eid ma clack shaw zupoven del ba" (February 7, 2018) {-}
https://statmodeling.stat.columbia.edu/2018/02/07/eid-ma-clack-shaw-zupoven-del-ba/

On variational inference diagnostics, mentions "clever second-order corrections (like the one proposed by Ryan Giordano, Tamara Broderick, and Michael Jordan)" for improving variational posteriors.

---


# AI-related posts on AG's Statistical Modeling blog (2022-2025)


<br/>

## 2022

### 1. "Chatbots: Still Dumb After All These Years" (January 2022) {-}
https://statmodeling.stat.columbia.edu/2022/01/
Discussion of chatbot limitations before the ChatGPT era.

<br/>

### 2. "A chatbot challenge for Blaise Agüera y Arcas and Gary Smith" (January 2022) {-}
Early discussion challenging claims about chatbot capabilities.

<br/>

### 3. "We have really everything in common with machine learning nowadays, except, of course, language" (May 13, 2022) {-}
https://statmodeling.stat.columbia.edu/2022/05/13/we-have-really-everything-in-common-with-machine-learning-nowadays-except-of-course-language/

Bob Carpenter had an interesting exchange with Andrew regarding the differences between statistics and machine learning, noting that "the two subfields also have different priorities and concepts" beyond just jargon differences.

<br/>

### 4. "From chatbots and understanding to appliance repair and statistical practice" (August 2022) {-}
Exploring the gap between AI claims and practical capabilities.

<br/>

### 5. "NeurIPS Ethics Code of Conduct" (May 2022) {-}
Discussion of the NeurIPS document "detailing a code of ethics, essentially listing concerns considered fair game for critiquing (possibly even rejecting) submitted papers" in machine learning.

<br/>

### 6. "The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning" (2022) {-}
Published paper by Jessica Hullman, Sayash Kapoor, Andrew Gelman, and others comparing methodological problems in psychology and ML.

<br/>

### 7. Bob Carpenter on the term "learning" (July 20, 2022) {-}
https://statmodeling.stat.columbia.edu/tag/ai/

"If I were the lexical police, I would've blacklisted the term 'learning,' because we're really just estimating parameters (aka 'weights') of a statistical model. I felt bad saying 'learning' even back when I worked in ML full time, but that's probably just because I lived through the AI winter during which the quickest route to rejection of a paper or grant was to mention 'artificial intelligence.'"

---

## 2023

### 8. "Bayesian statistics and machine learning: How do they differ?" (January 14, 2023) {-}
https://statmodeling.stat.columbia.edu/2023/01/14/bayesian-statistics-and-machine-learning-how-do-they-differ/

Gelman writes: "I associate machine learning with big models fit to big data and minimal assumptions: instead of assigning a structure to a model or using strong priors, you just use tons of training data... In contrast, I associate Bayesian inference with strong structures and strong priors. That is, the model is doing a lot of the work."

<br/>

### 9. "Predicting LLM havoc" (March 15, 2023) {-}
https://statmodeling.stat.columbia.edu/2023/03/15/predicting-llm-havoc/

Discussion of LLM deception: "What most captured my attention though is his argument about predictable deception, where a model fools or manipulates the (human) supervisor rather than doing the desired tasks, because doing so gets it better or equal reward."

<br/>

### 10. "AI as Wiley E. Coyote, and a funny thing about Worstfish" (June 2023)
AI skepticism and the gap between hype and reality.

<br/>

### 11. "Slides on large language models for statisticians" (July 26, 2023)
https://statmodeling.stat.columbia.edu/2023/07/26/slides-on-large-language-models-for-statisticians/

Bob Carpenter's tutorial on LLMs for statisticians, with discussion of ChatGPT's strengths in code generation and weaknesses in mathematics.

<br/>

### 12. "Artificial intelligence and aesthetic judgment" (August 16, 2023)
https://statmodeling.stat.columbia.edu/2023/08/16/artificial-intelligence-and-aesthetic-judgment/

Jessica Hullman, Ari Holtzman, and Andrew Gelman write: "Generative AIs produce creative outputs in the style of human expression. We argue that encounters with the outputs of modern generative AI models are mediated by the same kinds of aesthetic judgments that organize our interactions with artwork."

<br/>

### 13. "Report on the large language model meeting at Berkeley" (August 20, 2023)
https://statmodeling.stat.columbia.edu/2023/08/20/report-on-the-large-language-model-meeting-at-berkeley/

Bob Carpenter's extensive report on the Simons Institute workshop on LLMs and transformers. "'In-context learning' is what people call an LLM's ability to be given zero or more examples and then to complete the pattern... ChatGPT can manage all sorts of nuanced language tasks given only a few examples."

<br/>

### 14. "ChatGPT (4) can do 3-digit multiplication" (August 30, 2023)
https://statmodeling.stat.columbia.edu/2023/08/30/chatgpt-4-can-do-3-digit-multiplication/

Testing GPT-4's arithmetic capabilities and discussion of what LLMs can and cannot understand.

<br/>

### 15. "Beneath every application of causal inference to ML lies a ridiculously hard social science problem" (October 2, 2023)
https://statmodeling.stat.columbia.edu/2023/10/02/beneath-every-application-of-causal-inference-to-ml-lies-a-ridiculously-hard-social-science-problem/

Jessica Hullman discusses Zach Lipton's talk: "there's been considerable reflection lately on methods in machine learning, as it has become painfully obvious that accuracy on held-out IID data is often not a good predictor of model performance in a real-world deployment."

<br/>

### 16. "Frictionless reproducibility; methods as proto-algorithms; statisticians well prepared to think about issues raised by AI" (October 13, 2023)
https://statmodeling.stat.columbia.edu/2023/10/13/frictionless-reproducibility-methods-as-proto-algorithms-division-of-labor-as-a-characteristic-of-statistical-methods-statistics-as-the-science-of-defaults-statisticians-well-prepared-to-think-abo/

"Computer scientists have developed general tools (convolutional neural nets, autoencoders, normalizing flows, diffusions, attention and transformers) that can be widely applied to solve problems like natural language understanding, image recognition, protein folding..."

<br/>

### 17. "More than 10k scientific papers were retracted in 2023" (December 18, 2023)
https://statmodeling.stat.columbia.edu/2023/12/18/more-than-10k-scientific-papers-were-retracted-in-2023/

Discussion of how ChatGPT-generated papers are being detected and the impact on scientific publishing.

<br/>

### 18. "Explainable AI works, but only when we don't need it" (December 19, 2023)
https://statmodeling.stat.columbia.edu/2023/12/19/explainable-ai-works-but-only-when-we-dont-need-it/

Jessica Hullman reports on a NeurIPS talk by Ulrike Luxburg giving "a sort of impossibility result for explainable AI," distinguishing cooperative vs. adversarial scenarios for AI explanations and showing fundamental limitations of methods like SHAP and LIME.

---

## 2024

### 19. "Progress in 2023" (January 4, 2024)
https://statmodeling.stat.columbia.edu/2024/01/04/progress-in-2023/

Lists "Artificial intelligence and aesthetic judgment" among Andrew's publications for 2023.

<br/>

### 20. "International Cherry Blossom Prediction Competition with AI" (February 2024)
https://statmodeling.stat.columbia.edu/tag/ai/

"This year, contestants will not only compete against each other for the top prizes—but against artificial intelligence. We will include one or more submissions from the most popular large language models... Any human that beats the AI will receive commemorative memorabilia indicating they 'beat the bot in the 2025 International Cherry Blossom Prediction Competition.'"

<br/>

### 21. "Broader impact statements in machine learning papers" (February 28, 2024)
https://statmodeling.stat.columbia.edu/2024/02/28/

Jessica Hullman discusses NeurIPS requirements for broader impact statements, noting "Doubts about whether AI/ML researchers are qualified to be reflecting on the broader impacts of their work."

<br/>

### 22. "Re-reading Neuromancer in the age of AI" (2024)
https://statmodeling.stat.columbia.edu/tag/ai/

Phil revisits Gibson's 1984 novel: "In Neuromancer we encounter two types of artificial intelligence: artificial _general_ intelligence, as personified (machinified?) by AI's known as Wintermute and Neuromancer; and 'constructs'... Just think of the construct as an LLM that is trained to respond like a specific real person."

<br/>

### 23. "More red meat for you AI skeptics out there" (May 2024)
Continued documentation of AI limitations and hype.

<br/>

### 24. "Forking paths in LLMs for data analysis" (June 24, 2024)
https://statmodeling.stat.columbia.edu/2024/06/24/forking-paths-in-llms-for-data-analysis/

"LLMs are threatening because they would seem to make this kind of approach easier. For example, dealing with data formatting issues remains a major time-sink in analysis, especially for users who are not programmers, but ChatGPT at least superficially seems tolerant of whatever you want to paste in."

<br/>

### 25. "NeurIPS 2024 workshop on Statistical Frontiers in LLMs and Foundation Models" (August 9, 2024)
https://statmodeling.stat.columbia.edu/2024/08/09/neurips-2024-workshop-on-statistical-frontiers-in-llms-and-foundation-models/

Jessica Hullman announces a workshop on "statistical methods development for LLMs and foundation models," covering topics including "Watermarking... for identifying AI-generated content," "Conformal prediction and black-box uncertainty quantification," and "Auditing, safety, and risk analysis."

<br/>

### 26. "New Course: Prediction for (Individualized) Decision-making" (December 6, 2024)
https://statmodeling.stat.columbia.edu/2024/12/06/new-course-prediction-for-individualized-decision-making/

Course syllabus with extensive AI-related readings including papers on AI-assisted decision making, AI reliance, and human-AI hybrid systems.

---

## 2025

### 27. "Large Language Models (LLMs) Flunk the Word Game Connections" (January 2025)
https://statmodeling.stat.columbia.edu/

Gary Smith's experiments showing "LLMs are undeniably astonishingly good at using the text they trained on (aided by human fine tuners) to generate convincing prose. But they are really bad at distinguishing between truth and falsehoods and responding to prompts that are unlike or even slightly different from what they trained on."

<br/>

### 28. "Progress in 2024 (Aki)" (January 7, 2025)
https://statmodeling.stat.columbia.edu/2025/01/07/progress-in-2024-aki/

Aki Vehtari's annual progress report, including discussion of AI tools for coding efficiency.

---

# Key Recurring Themes Across Posts

1. **AI Hype vs. Reality**: Gelman and collaborators consistently document the gap between AI marketing claims and actual capabilities, particularly Gary Smith's critiques

2. **LLM Limitations**: Multiple posts explore what ChatGPT/GPT-4 can and cannot do (arithmetic, reasoning, understanding vs. pattern matching)

3. **Statistics-ML Translation**: Several posts explore how statistical concepts translate (or don't) to machine learning terminology

4. **Explainable AI Skepticism**: Jessica Hullman's posts express skepticism about XAI methods and their practical utility

5. **AI in Scientific Practice**: Concerns about AI-generated papers, research fraud detection, and AI's role in data analysis

6. **Philosophical Questions**: Posts exploring whether LLMs "understand" anything, consciousness, and the distinction between AI and AGI

---

# Sources

## Terry Speed Sources

- WEHI Researcher Profile: https://www.wehi.edu.au/researcher/terry-speed/
- Wikipedia: https://en.wikipedia.org/wiki/Terry_Speed
- IMS Teaching Statistics article: https://imstat.org/2017/10/02/teaching-statistics-in-the-age-of-data-science/
- AMSTAT Interview 2016: https://magazine.amstat.org/blog/2016/09/01/terryspeedinterview/
- WEHI AI article 2023: https://www.wehi.edu.au/news/leaping-into-the-digital-future-ai-at-wehi/

## Andrew Gelman's Blog: 

- Main site: https://statmodeling.stat.columbia.edu/
- AI tag: https://statmodeling.stat.columbia.edu/tag/ai/
- ChatGPT tag: https://statmodeling.stat.columbia.edu/tag/chatgpt/
- LLM tag: https://statmodeling.stat.columbia.edu/tag/llm/

<!--

<br/>

 References 

<div id="refs"></div>

-->

<br/>


```{r, echo=FALSE}
  knitr::knit_exit()
```

###########################################################
 ARCHIVED CODE BELOW
###########################################################


    
<!-- To run
# nohup Rscript -e "knitr::knit2html('index.Rmd')" > index.log  &
    
# Or 
# nohup Rscript -e "rmarkdown::render('index.Rmd')" > index.log  &
    
-->
